{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Trials\n",
    "\n",
    "Repeated independent trials are called Bernoulli trials if there are only two possible outcomes for each trial and their probabilities remain the same throughout the trials. It is usual to denote the two probabilities by $p$ and $q$, and to refer to the outcome with probability $p$ as success, $S$, and to the other as failure, $F$. Clearly, $p$ and $q$ must be non-negative and \n",
    "\n",
    "\\begin{align*}\n",
    "p + q = 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample space of each individual trial is formed by the two points $S$ and $F$. The sample space of $n$ Bernoulli trials contains $2^n$ points or successions of $n$ symbols $S$ and $F$, each point representing one possible outcome of the compound experiment. Since the trials are independent, the probabilities multiply. In other words, *the probability of any specified sequence is the product obtained on replacing the symbols $S$ and $F$ by $p$ and $q$* respectively.  Thus, $P\\{SSFSF \\ldots FFS\\} = ppqpq\\ldots qqp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Binomial Distribution\n",
    "\n",
    "Frequently, we are interested only in the total number of successes produced in a succession of $n$ Bernoulli trials but not in their order. The number of successes can be $0,1,2\\ldots,n$ and our first problem is to determine the corresponding probabilities. Now, the event $n$ trials result in $k$ successes and $n-k$ failures can happen in as many ways as $k$ letters $S$ can be distributed among $n$ places. In other words, our event contains ${n \\choose k}$ points, and, by definition, each point has the probability $p^k q^{n-k}$. This proves the\n",
    "\n",
    "---\n",
    "**Theorem.** Let $b(k;n,p)$ be the probability that $n$ Bernoulli trails with probabilities $p$ for success and $q = 1-p$ for failure result in $k$ successes and $n-k$ failures. Then,\n",
    "\n",
    "\\begin{equation*}\n",
    "b(k;n,p) = {n \\choose k} p^k q^{n - k} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "---\n",
    "\n",
    "In particular, the probability of no success is $q^n$ and the probability of atleast one success is $1-q^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall treat $p$ as a constant and denote the number of successes in $n$ trilas by $S_n$; then $b(k;n,p) = P\\{S_n = k\\}$. In the general terminology, $S_n$ is a random variable, and the function $b(l;n,p)$ is the PMF of this random variable; we shall refer to it as the binomial PMF. The attribute binomial refers to the fact that equation (1) represents the $k$th term of the binomial expansion of $(q+p)^n$. This remark also shows that \n",
    "\n",
    "\\begin{align*}\n",
    "b(0;n,p) + b(1;n,p) + \\ldots + b(n;n,p) = (q+p)^n = 1\n",
    "\\end{align*}\n",
    "\n",
    "as is required by the notion of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Term and the tails\n",
    "\n",
    "From equation (1), we see that\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{b(k;n,p)}{b(k-1;n,p)} &= \\frac{{n \\choose k}p^k q^{n-k}}{{n \\choose {k-1}}p^{k-1} q^{n-k+1}} = \\frac{\\frac{n!}{k!(n-k)!}\\cdot p}{\\frac{n!}{(k-1)!(n-k+1)!}\\cdot q}\\\\\n",
    "&= \\frac{p}{q} \\left(\\frac{n - k + 1}{k}\\right) = \\frac{(n+1)p - pk + qk - qk}{qk}\\\\\n",
    "&= 1 + \\frac{(n+1)p - k}{qk}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, the term $b(k;n,p)$ is greater than the preceding one for $(n+1)p > k$, that is $k < (n+1)p$ and is small for $k > (n+1)p$. If $(n+1)p = m$ happens to be an integer, then $b(m;n,p) = b(m-1;n,p)$. Thus, there exists exactly one integer $m$ such that,\n",
    "\n",
    "\\begin{equation*}\n",
    "(n+1)p - 1 < m \\le (n+1)p \\tag{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Theorem.** As $k$ goes from $0$ to $n$, the terms $b(k;n,p)$ first inrease monotonically, then decrease monotonically, reaching their greatest value when $k = m$, except that $b(m-1;n,p) = b(m;n,p)$ when $m = (n+1)p$. \n",
    "\n",
    "---\n",
    "\n",
    "We shall call $b(m;n,p)$ the *central* term. Often $m$ is called the most probable number of successes, but it must be understood that for large values of $n$, all terms $b(k;n,p)$ are small. In 100 tossings of a true coin, the most probable number of heads is 50, but its probability is less than $0.09$. In the next chapter we shall find that $b(m;n,p)$ is approximately $1/\\sqrt{2\\pi npq}$.\n",
    "\n",
    "Consider the sequence $(a_k)$ whose terms are ratio of the binomial coefficients $\\frac{b(k;n,p)}{b(k-1;n,p)}$, we saw above. We have, $a_k = \\frac{(n+1)p}{kq}-\\frac{p}{q}$. Clearly therefore, as $k$ increases, the ratio $a_k$ decreases monotonically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $k \\geq (r+1)$, then, $(n+1-k)p \\leq (n+1-(r+1))p = (n-r)p$. And $qk \\geq q(r+1)$. So, $\\frac{1}{qk} \\leq \\frac{1}{(r+1)q}$. So, we have an upper bound on $\\frac{b(k;n,p)}{b(k-1;n,p)}$.\n",
    "\n",
    "When $k \\geq (r+1)$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{b(k;n,p)}{b(k-1;n,p)} \\leq \\frac{(n-r)p}{(r+1)q} \\tag{3}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set herein $k=r+1,\\ldots,r+\\nu$ and multiply the $\\nu$ inequalities to obtain\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{b(r+\\nu;n,p)}{b(r+\\nu-1;n,p)} \\cdot \\frac{b(r+\\nu-1;n,p)}{b(r+\\nu-2;n,p)} \\cdots \\frac{b(r+1;n,p)}{b(r;n,p)} \\le \\left\\{\\frac{(n-r)p}{(r+1)q}\\right\\}^{\\nu}\n",
    "\\end{align*}\n",
    "\n",
    "Consequently,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{b(r+\\nu;n,p)}{b(r;n,p)} \\leq \\left\\{\\frac{(n-r)p}{(r+1)q}\\right\\}^{\\nu} \\tag{4}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $r \\geq np$, let's find an upper bound for the fraction $\\frac{(n-r)p}{(r+1)q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, if $r \\geq np$, $(n-r)p \\leq (n-np)p = npq$. So,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{(n-r)p}{(r+1)q} \\leq \\frac{npq}{(r+1)q} = \\frac{np}{r+1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we want a lower bound on the denominator. $(r+1) \\geq (np + 1)$. So, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{(n-r)p}{(r+1)q} \\leq \\frac{np}{np+1} = 1 - \\frac{1}{np + 1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for $r \\geq np$, the fraction $\\frac{(n-r)p}{(r+1)q}$ is strictly less than unity, and the summation over $\\nu$ leads to a finite geometric series with the ratio $\\frac{(n-r)p}{(r+1)q}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\sum_{\\nu = 0}^{(n-r)} \\left(\\frac{(n-r)p}{(r+1)q}\\right)^{\\nu} &\\leq \\sum_{\\nu = 0}^{\\infty} \\left(\\frac{(n-r)p}{(r+1)q}\\right)^{\\nu} = \\frac{1}{1-\\frac{(n-r)p}{(r+1)q}} \\\\\n",
    "&= \\frac{(r+1)q}{(r+1)q-(n-r)p}  \\\\\n",
    "&= \\frac{(r+1)q}{(r+1)q-((n+1)-(r+1))p} = \\frac{(r+1)q}{(r+1)q-(n+1)p+(r+1)p}\\\\\n",
    "&= \\frac{(r+1)q}{(r+1)-(n+1)p}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that for $r \\geq np$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{\\nu=0}^{(n-r)} \\frac{b(r+\\nu;n,p)}{b(r;n,p)} \\leq \\frac{(r+1)q}{(r+1)-(n+1)p}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{\\nu=0}^{(n-r)}b(r+\\nu;n,p) \\leq b(r;n,p)\\cdot\\frac{(r+1)q}{(r+1)-(n+1)p} \\tag{5}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left, we have the **right tail** of the binomial distribution, namely the probability of atleast $r$ successes. The same calculation applied to the left tail shows that for $s \\leq np$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{\\rho=0}^{s}b(\\rho;n,p) \\leq b(s;n,p)\\cdot\\frac{(n-s+1)p}{(n+1)p - s} \\tag{6}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Theorem.** If $r \\geq np$, the probability of atleast $r$ successes satisfies the inequality (5); if $s \\leq np$, the probability of at most $s$ successes satisfies inequality (6).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law Of Large Numbers\n",
    "\n",
    "On several occasions, we have mentioned that our intuitive notion of probability is based on the following assumption. If in $n$ identical trails, $A$ occurs $\\nu$ times, and if $n$ is very large, then $\\nu/n$ should be near the probability of $A$. Clearly, a formal mathematical theory can never refer directly to real life, but it should atleast provie theoretical counterparts to the phenomena, which it tries to explain. Accordingly, we require that the vague introductory remark be made precise in the form a theorem. For this purpose we translate *identical trials* as *Bernoulli trials* with probability $p$ for success. If $S_n$ is the number of successes in $n$ trials, then $S_n/n$ is the average number of successes and should be near $p$. It is now easy to give a precise meaning to this. Pick an arbitrary $\\epsilon > 0$. Consider for example, the probability that $\\frac{S_n}{n} - p$ exceeds $\\epsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probability is the same as $P\\{S_n > n(p + \\epsilon)\\}$ and equals the left side of the inequality (5), when $r \\geq n(p + \\epsilon)$. Then, we can find an upper bound for the fraction $\\frac{(r+1)q}{(r+1)-(n+1)p}$. Since $r \\leq n$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{(r+1)q}{(r+1)-(n+1)p} &\\leq \\frac{(n+1)q}{(n(p+\\epsilon)+1)-(n+1)p}\\\\\n",
    "&=\\frac{(n+1)q}{np + n\\epsilon + 1 - np - p} \\\\\n",
    "&=\\frac{(n+1)q}{n\\epsilon + q}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we proved that the $n$th term of the sequence $(p_n)$ has an upper bound:\n",
    "\n",
    "\\begin{align*}\n",
    "P\\{S_n > n(p + \\epsilon)\\} \\leq b(r;n,p) \\cdot \\frac{(n+1)q}{n\\epsilon + q}\n",
    "\\end{align*}\n",
    "\n",
    "Since probabilities are non-negative,\n",
    "\n",
    "\\begin{align*}\n",
    "0 \\leq P\\{S_n > n(p + \\epsilon)\\} \\leq b(r;n,p) \\cdot \\frac{(n+1)q}{n\\epsilon + q}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing to the limit, as $n \\to \\infty$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lim 0 \\leq \\lim P\\{S_n > n(p + \\epsilon)\\} \\leq \\lim \\left[ b(r;n,p) \\cdot \\frac{(n+1)q}{n\\epsilon + q}\\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,\n",
    "\n",
    "\\begin{align*}\n",
    "\\lim \\left[b(r;n,p) \\cdot \\frac{(n+1)q}{n\\epsilon + q}\\right] &= \\lim b(r;n,p) \\cdot \\lim \\frac{\\left(1+\\frac{1}{n}\\right)q}{\\epsilon + \\frac{q}{n}}\\\\\n",
    "&= \\lim b(r;n,p) \\cdot \\frac{q}{\\epsilon}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to see, that as $n \\to \\infty$, $b(r;n,p) \\to 0$. Note that, as $n \\to \\infty$, $r \\to \\infty$, because $r \\geq n(p + \\epsilon)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\lim b(r;n,p) &= \\lim \\frac{n!}{(n-r)!r!} \\cdot \\lim p^r q^{n-r}\\\\\n",
    "&\\leq \\lim \\frac{n!}{n!0!} \\cdot \\lim p^n q^n \\quad \\{ r \\ge 0 \\text{ and } r \\leq n \\} \\\\\n",
    "&= \\lim p^n q^n \\{ \\text{ since } (q^n) \\to 0, \\text{ if } |q|<1 \\}\\\\\n",
    "&= 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Squeeze theorem, \n",
    "\n",
    "\\begin{align*}\n",
    "\\lim P\\{S_n > n(p + \\epsilon)\\} = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the inequality (6), we see in the same way that $P\\{S_n < n(p - \\epsilon)\\} \\to 0$, and we have thus \n",
    "\n",
    "\\begin{align*}\n",
    "P\\left\\{\\left\\lvert\\frac{S_n}{n} - p\\right\\rvert < \\epsilon\\right\\} \\to 1 \\tag{8}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In words: As $n$ increases, the probability that the average number of successes deviates from $p$ by more than any preassigned $\\epsilon$ tends to zero.* This is one form of the law of large numbers and serves as a basis for the intuitive notion of probability as a measure of relative frequencies. For practical applications it must be supplemented by a more precise estimate of the probability on the left side in (8); such an estimate is provided by the normal approximation to the binomial distribution. \n",
    "\n",
    "The assertion (8) is the classical law of large numbers. It is of very limited interest and should be replaced by the more previse and more useful strong law of large numbers.\n",
    "\n",
    "*Warning.* It is usual to read into the law of large numbers things which it definitely does not imply. If Peter and Paul toss a perfect coin 10,000 times, it is customary to expect that Peter will lead roughly half the time. **This is not true.** The arc sine law states that such an equalization is least probable. The probability that Peter leads in less than $20$ trials is very much larger than the probability that the number of trials in which he leads lies between 4990 and 5010. There does not exist any tendency for the periods of lead to equalize. The law of large numbers asserts only that in a a large number of different coin tossing games, the frequency of those in which heads lead is, at any given moment, close to $\\frac{1}{2}$. Nothing is said about the fluctuations of the lead within a fixed game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poisson Approximation\n",
    "\n",
    "In many applications, we deal with Bernoulli trials where, comparatively speaking $n$ is large and $p$ is small, whereas the product \n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda = np \\tag{9}\n",
    "\\end{align*}\n",
    "\n",
    "is of modertate magnitude. In such cases, it is convenient to use an approximation formula to $b(k;n,p)$ which is due to Poisson and which we proceed to derive. We have $b(0;n,p) = (1 - p)^n$ or substituting from (9),\n",
    "\n",
    "\\begin{equation*}\n",
    "b(0;n,p) = \\left(1 - \\frac{\\lambda}{n}\\right)^n \\tag{10}\n",
    "\\end{equation*}\n",
    "\n",
    "Taking the logarithm on both sides:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log b(0;n,p) = n \\log\\left(1 - \\frac{\\lambda}{n}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Taylor's series for $\\log (1 + x)$ at $x = 0$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log (1 + x) &= f(0) + xf'(0) + \\frac{x^2}{2!}f''(0) + \\frac{x^3}{3!}f'''(0) + \\ldots\\\\\n",
    "&= x + \\frac{x^2}{2!}\\cdot (-1) + \\frac{x^3}{3!} \\cdot 2 + \\frac{x^4}{4!}\\cdot (-3 \\cdot 2 \\cdot 1) \\\\\n",
    "&= x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\ldots\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, \n",
    "\n",
    "\\begin{align*}\n",
    "\\log b(0;n,p) &= n \\log\\left(1- \\frac{\\lambda}{n}\\right) = n\\left(-\\frac{\\lambda}{n}-\\frac{\\lambda^2}{2n^2}-\\ldots\\right)\\\\\n",
    "&= -\\lambda - \\frac{\\lambda^2}{2n} - \\frac{\\lambda^3}{3n^2} - \\ldots \\tag{11}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $n \\to \\infty$, $\\lim \\log b(0;n,p) = - \\lambda$, so that for large $n$,\n",
    "\n",
    "\\begin{align*}\n",
    "b(0;n,p) \\approx e^{-\\lambda} \\tag{12}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the sign $\\approx$ is used to indicate approximate equality. Furthermore, from the expression for $b(k;n,p)/b(k-1;n,p)$, it is seen that for any fixed $k$ and sufficiently large $n$ we have, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{b(k;n,p)}{b(k-1;n,p)} &= \\frac{(n-k+1)p}{kq}\\\\\n",
    "&=\\frac{np-(k-1)p}{kq}\\\\\n",
    "&= \\frac{\\lambda - (k-1)p}{kq}\n",
    "\\end{align*}\n",
    "\n",
    "For very small values of $p$, we can write $p \\approx 0$ and $q \\approx 1$. Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{b(k;n,p)}{b(k-1;n,p)} \\approx \\frac{\\lambda}{k} \\tag{13}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k=1$, we get from this and (12), $b(1;n,p)\\approx \\lambda e^{-\\lambda}$. For $k=2$, we get $b(2;n,p) \\approx \\frac{\\lambda^2}{2}e^{-\\lambda}$. Generally, we see by induction that,\n",
    "\n",
    "\\begin{equation*}\n",
    "b(k;n,p) \\approx \\frac{\\lambda^k}{k!} e^{-\\lambda} \\tag{14}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the famous *Poisson approximation to the binomial distribution*. (See problems 30-34 for an estimate of the error and a proof that the approximation in (14) is uniform when $n \\to \\infty$ and $p \\to 0$ in such a way that $\\lambda = np$ remains bounded.) It is convenient to have a symbol for the right-hand member in (14) and we shall put\n",
    "\n",
    "\\begin{align*}\n",
    "p(k;\\lambda) = \\frac{\\lambda^k}{k!}e^{-\\lambda} \\tag{15}\n",
    "\\end{align*}\n",
    "\n",
    "With this notation $p(k;\\lambda)$ should be an approximation to $b(k;n,\\lambda/n)$ when $n$ is sufficiently large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function p(k,λ)\n",
    "    ((λ^k)/factorial(k)) * ℯ^(-λ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Birthdays*. What is the probability, $p_k$, that in a company of $500$ people exactly $k$ will have birthdays on New Year's day? \n",
    "\n",
    "If the $500$ people are cosen at random, we may apply the scheme of 500 Bernoulli trials with the probability of success $p = \\frac{1}{365}$. Then, $p_0 = \\left(\\frac{364}{365}\\right)^{500} \\approx 0.2537$. For the Poisson approximation, we put $\\lambda = \\frac{500}{365}$. Then, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(0,1.369863) = 0.254142\n",
      "p(1,1.369863) = 0.348139\n",
      "p(2,1.369863) = 0.238452\n",
      "p(3,1.369863) = 0.108882\n",
      "p(4,1.369863) = 0.037288\n",
      "p(5,1.369863) = 0.010216\n",
      "p(6,1.369863) = 0.002332\n"
     ]
    }
   ],
   "source": [
    "using Printf\n",
    "\n",
    "λ = 500/365\n",
    "\n",
    "for k = 0:6\n",
    "    @printf(\"p(%d,%f) = %f\\n\",k,λ,p(k,λ))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Centenarians.* At birth any particular person has a small chance of living 100 years, and in a large community the number of yearly births are large. Owing to wars, epidemics etc. different lives are not stochastically independent, but as a first approximation we may compare $n$ births to $n$ Bernoulli trials with death after $100$ years as success. In a stable community, where neither size nor mortality rate appreciably, it is reasonable to expect that the frequency of years in which exactly $k$ centenarians die is approximately $p(k;\\lambda)$, with $\\lambda$ depending on the size and health of the community.\n",
    "\n",
    "Intuitively, there will be years in which exactly $1$ centenarian dies, there will be years in which $2$ centenarians die and there will be years in which exactly $k$ centenarians die. $p(k,\\lambda)$ is on an average the count(number) of 365-day years in which $k$ centenarians die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Misprints, Raisins*. If in printing a book there is a constant probability of any letter's being misprinted, and if the conditions of printing remain unchanged, then we have as many Bernoulli trials as there are letters. The frequency of the pages containing exactly $k$ misprints will then be approximately $p(k;\\lambda)$ where $\\lambda$ is the characteristic of the printer. Thus, the Poisson formula may be used to discover radical departures from uniformity or from the state of statistical control. A similar agrument applies in many cases. For example, if many raisins are distributed in the dough, we should expect that thorough mixing will result in the frequency of loaves with exactly $k$ raisin to be approximately $p(k;\\lambda)$ with $\\lambda$ a measure of the density of raisins in the dough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poisson Distribution\n",
    "\n",
    "In the preceding section, we have used the Poisson expression merely as a convenient approximation to the binomial distribution in the case of large $n$ and small $p$. In connection with the matching and occupany problems of chapter IV, we have studied different probability distributions, which have also led to the Poisson expressions $p(k;\\lambda)$ as a limiting form. We have here a special case of the remarkable fact that there exist a few distributions of great universaility that occur in a surprisingly large variety of problems. The three principal distributions, with ramifications throughout proability theory are the binomial distribution, the normal distribution (to be introduced in the following chapter), and the Poisson distribution\n",
    "\n",
    "\\begin{align*}\n",
    "p(k;\\lambda) = \\frac{\\lambda^k}{k!}e^{-\\lambda} \\tag{16}\n",
    "\\end{align*}\n",
    "\n",
    "which we shall now consider on its own merits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note first that on adding the equations (16) for $k = 0,1,2,\\ldots$ we get on the right side $e^{-\\lambda}$ times the Taylor's series for $e^\\lambda$. Hence, for any fixed $\\lambda$, the quantities $p(k;\\lambda)$ add to unity, and therefore it is possible to conceive of an ideal experiment in which $p(k;\\lambda)$ is the probability of exactly $k$ successes. We shall now indicate my many physical experiments and statistical observations actually lead to such an interpretation of (16). The examples of the next section will illustrate the wide range and the importance of the various applications of (16). The true nature of the Poisson distribution will become apparent only in connection with the theory of stochastic processes. \n",
    "\n",
    "Consider a sequence of random events occurring in time, such as, radioactive disintegrations or incoming calls at a telephone exchange. Each event is represented by a point on the time axis, and we are concerned with chance distributions of points. There exists many different types of such distributions, but their study belongs to the domain of continuous probabilities which we have postponed to the second volume. Here, we shall be content to show that the simplest physical assumptions lead to $p(k;\\lambda)$ as the probability of finding exactly $k$ points (events) within a fixed interval of specified length. Our methods are necessarily crude, and we shall return to the same problem with more adequate methods in chapter XVII.\n",
    "\n",
    "The physical assumptions which we want to express mathematically are that the conditions of the experiment remain constant in time, and that non-overlapping time intervals are stochastically independent in the sense that information concerning the number of evernts in one interval reveals nothing about the other. The theory of probabilities in a continuum makes it possible to express these statements directly, but being restricted to discrete probabilities, we have to an approximate finite model and pass to the limit. \n",
    "\n",
    "Imagine that the unit time interval divided into a great number $n$ of intervals, each of length $1/n$. Either a particular subinterval is empty or it contains atleast one of our random points(or events) and we agree to call the two possibilities failure and success, respectively. The probability $p_n$ of success must be the same for all $n$ subintervals, since they have the same length. The assumed independence of non-overlapping intervals then implies that we have $n$ Bernoulli trials and the probability of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
